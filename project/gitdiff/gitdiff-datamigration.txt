diff --git a/api/models.py b/api/models.py
index 0a9e908..58765ef 100644
--- a/api/models.py
+++ b/api/models.py
@@ -1,5 +1,7 @@
 from pydantic import BaseModel
-from typing import List
+from typing import List, Optional
+from datetime import datetime
+from uuid import UUID
 
 class ConversionInput(BaseModel):
     sql: str
@@ -59,3 +61,27 @@ class SqlExecutionJob(BaseModel):
     submitted_at: str
     processed_at: str | None = None
     statement_results: list[dict] | None = None
+
+class DataMigrationJob(BaseModel):
+    job_id: UUID
+    source_db_type: str
+    source_connection_string: str
+    source_table_name: str
+    target_db_type: str
+    target_connection_string: str
+    target_table_name: str
+    status: str
+    started_at: datetime
+    completed_at: Optional[datetime] = None
+    total_rows: int = 0
+    migrated_rows: int = 0
+    failed_rows: int = 0
+    error_details: Optional[str] = None
+
+class DataMigrationRow(BaseModel):
+    row_id: UUID
+    job_id: UUID
+    source_primary_key_value: Optional[str] = None
+    status: str
+    error_message: Optional[str] = None
+    migrated_at: datetime
diff --git a/api/queues.py b/api/queues.py
index 19c8f30..65ec035 100644
--- a/api/queues.py
+++ b/api/queues.py
@@ -16,6 +16,12 @@ QUEUE_CONFIG['SQL_EXECUTION'] = {
     'dlq': 'sql_execution_dlq',
 }
 
+QUEUE_CONFIG['DATA_MIGRATION_ROW'] = {
+    'queue': 'data_migration_row_jobs',
+    'dlx': 'data_migration_row_dlx',
+    'dlq': 'data_migration_row_dlq',
+}
+
 def get_rabbitmq_connection():
     try:
         connection = pika.BlockingConnection(
@@ -24,4 +30,17 @@ def get_rabbitmq_connection():
         return connection
     except pika.exceptions.AMQPConnectionError as e:
         print(f"Failed to connect to RabbitMQ: {e}")
-        return None
\ No newline at end of file
+        return None
+
+def declare_quorum_queue(channel, queue_name, dlx_name):
+    channel.exchange_declare(exchange=dlx_name, exchange_type='fanout')
+    channel.queue_declare(queue=f"{queue_name}_dlq", durable=True, arguments={'x-queue-type': 'quorum'})
+    channel.queue_bind(exchange=dlx_name, queue=f"{queue_name}_dlq")
+    channel.queue_declare(
+        queue=queue_name,
+        durable=True,
+        arguments={
+            'x-queue-type': 'quorum',
+            'x-dead-letter-exchange': dlx_name
+        }
+    )
\ No newline at end of file
diff --git a/api/routes/migration_routes.py b/api/routes/migration_routes.py
index f109396..5120ffa 100644
--- a/api/routes/migration_routes.py
+++ b/api/routes/migration_routes.py
@@ -1,61 +1,91 @@
 from fastapi import APIRouter, HTTPException
 import os
 import pika
-import redis
 import json
 import uuid
 from .. import models
+from .. import schema_comparer
+from .. import migration_db
+from .. import queues
 
 router = APIRouter()
 
-def get_redis_connection():
-    return redis.Redis(host=os.getenv("REDIS_HOST", "localhost"), port=int(os.getenv("REDIS_PORT", 6379)), db=0, decode_responses=True)
-
 @router.post("/migrate/start")
 def start_migration(request: models.DataMigrationRequest):
     if not all([request.oracle_credentials, request.postgres_credentials, request.source_table, request.destination_table]):
         raise HTTPException(status_code=400, detail="All fields are required.")
 
-    task_id = str(uuid.uuid4())
-    task_info = {
-        "task_id": task_id,
-        "oracle_credentials": request.oracle_credentials.dict(),
-        "postgres_credentials": request.postgres_credentials.dict(),
-        "source_table": request.source_table,
-        "destination_table": request.destination_table
-    }
+    # 1. Perform Schema Compatibility Check
+    try:
+        oracle_schema, postgres_schema = schema_comparer.get_table_schemas(
+            request.oracle_credentials,
+            request.source_table,
+            request.oracle_credentials.service_name or request.oracle_credentials.sid, # Assuming schema name is service_name/sid for simplicity
+            request.postgres_credentials,
+            request.destination_table
+        )
+        is_compatible, issues = schema_comparer.compare_schemas(oracle_schema, postgres_schema)
+
+        if not is_compatible:
+            raise HTTPException(
+                status_code=400,
+                detail={"message": "Schema incompatibility detected", "issues": issues}
+            )
+    except Exception as e:
+        raise HTTPException(status_code=500, detail=f"Schema comparison failed: {e}")
 
+    # 2. Create a new migration job in PostgreSQL
     try:
-        connection = pika.BlockingConnection(pika.ConnectionParameters(host=os.getenv('RABBITMQ_HOST', 'localhost')))
+        job_id = migration_db.create_migration_job(
+            source_db_type="Oracle",
+            source_connection_string=json.dumps(request.oracle_credentials.dict()),
+            source_table_name=request.source_table,
+            target_db_type="PostgreSQL",
+            target_connection_string=json.dumps(request.postgres_credentials.dict()),
+            target_table_name=request.destination_table
+        )
+    except Exception as e:
+        raise HTTPException(status_code=500, detail=f"Failed to create migration job: {e}")
+
+    # 3. Publish message to RabbitMQ for worker to pick up
+    try:
+        connection = queues.get_rabbitmq_connection()
+        if not connection:
+            raise HTTPException(status_code=500, detail="Failed to connect to RabbitMQ.")
         channel = connection.channel()
+
+        # Use the existing data_migration_jobs queue for initial job dispatch
+        # The worker will then break it down into row-level jobs
+        queue_config = queues.QUEUE_CONFIG['DATA_MIGRATION_ROW'] # This is for row-level, but the initial job is still 'data_migration_jobs'
+        # The original data_migration_jobs queue is declared in worker.py main function
+        # For now, we'll use a hardcoded queue name for the initial job dispatch
+        # TODO: Refactor worker.py to use QUEUE_CONFIG for data_migration_jobs as well
         channel.queue_declare(queue='data_migration_jobs', durable=True, arguments={'x-queue-type': 'quorum'})
+
+        message = {"job_id": str(job_id)}
         channel.basic_publish(
             exchange='',
             routing_key='data_migration_jobs',
-            body=json.dumps(task_info),
+            body=json.dumps(message),
             properties=pika.BasicProperties(delivery_mode=2) # make message persistent
         )
         connection.close()
 
-        # Set initial status in Redis
-        redis_conn = get_redis_connection()
-        initial_status = {"status": "submitted", "message": "Task has been submitted to the queue."}
-        redis_conn.set(f"migration_status:{task_id}", json.dumps(initial_status), ex=3600) # 1-hour expiry
+        migration_db.update_migration_job_status(job_id, "SUBMITTED")
 
-        return {"task_id": task_id}
+        return {"job_id": job_id, "message": "Migration job submitted successfully."}
     except Exception as e:
-        raise HTTPException(status_code=500, detail=f"Error submitting task: {e}")
+        migration_db.update_migration_job_status(job_id, "FAILED", error_details=str(e))
+        raise HTTPException(status_code=500, detail=f"Error submitting migration job to queue: {e}")
 
-@router.get("/migrate/status/{task_id}")
-def check_status(task_id: str):
-    if not task_id:
-        raise HTTPException(status_code=400, detail="Task ID is required.")
+@router.get("/migrate/status/{job_id}")
+def check_status(job_id: str):
+    if not job_id:
+        raise HTTPException(status_code=400, detail="Job ID is required.")
     try:
-        redis_conn = get_redis_connection()
-        status_json = redis_conn.get(f"migration_status:{task_id}")
-        if status_json:
-            status_data = json.loads(status_json)
-            return status_data
-        return {"status": "pending", "message": ""}
+        job = migration_db.get_migration_job(job_id)
+        if job:
+            return job.dict()
+        raise HTTPException(status_code=404, detail="Migration job not found.")
     except Exception as e:
-        raise HTTPException(status_code=500, detail=f"Error checking status: {e}")
+        raise HTTPException(status_code=500, detail=f"Error checking migration job status: {e}")
diff --git a/api/startup.py b/api/startup.py
index b9b092a..b776702 100644
--- a/api/startup.py
+++ b/api/startup.py
@@ -2,6 +2,7 @@ from fastapi import FastAPI
 import pika
 from . import database
 from . import queues
+from . import migration_db
 from contextlib import asynccontextmanager
 
 @asynccontextmanager
@@ -10,6 +11,7 @@ async def lifespan(app: FastAPI):
     database.create_jobs_table()
     database.create_ddl_jobs_table()
     database.create_sql_execution_jobs_table()
+    migration_db.create_migration_tables()
     
     # Declare queues on startup
     connection = queues.get_rabbitmq_connection()
@@ -27,17 +29,7 @@ def declare_queues(connection):
         channel = connection.channel()
         for object_type in queues.QUEUE_CONFIG:
             config = queues.QUEUE_CONFIG[object_type]
-            channel.exchange_declare(exchange=config['dlx'], exchange_type='fanout')
-            channel.queue_declare(queue=config['dlq'], durable=True, arguments={'x-queue-type': 'quorum'})
-            channel.queue_bind(exchange=config['dlx'], queue=config['dlq'])
-            channel.queue_declare(
-                queue=config['queue'],
-                durable=True,
-                arguments={
-                    'x-queue-type': 'quorum',
-                    'x-dead-letter-exchange': config['dlx']
-                }
-            )
+            queues.declare_quorum_queue(channel, config['queue'], config['dlx'])
         channel.close()
     except pika.exceptions.AMQPError as e:
         print(f"Failed to declare queues: {e}")
diff --git a/worker.py b/worker.py
index 4858d81..8bbdfd5 100644
--- a/worker.py
+++ b/worker.py
@@ -10,7 +10,6 @@ import sys
 import os
 import psycopg2
 import oracledb
-import redis
 import uuid
 import sqlparse
 from typing import Optional
@@ -18,7 +17,7 @@ from typing import Optional
 sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '.')))
 
 WORKER_ID = str(uuid.uuid4())[:8]
-from api import database, queues, ai_converter
+from api import database, queues, ai_converter, migration_db, schema_comparer
 
 # --- Existing SQL Conversion Feature ---
 
@@ -84,20 +83,8 @@ def callback(ch, method, properties, body):
 
 # --- New Data Migration Feature ---
 
-DATA_MIGRATION_DLX = 'data_migration_dlx'
-DATA_MIGRATION_DLQ = 'data_migration_dlq'
-DATA_MIGRATION_QUEUE = 'data_migration_jobs'
 CHUNK_SIZE = 1000
 
-def get_redis_connection():
-    """Establishes a connection to Redis."""
-    return redis.Redis(
-        host=os.getenv("REDIS_HOST", "localhost"),
-        port=int(os.getenv("REDIS_PORT", 6379)),
-        db=0,
-        decode_responses=True
-    )
-
 def get_oracle_connection(user, password, host, port, service_name):
     """Establishes a connection to the Oracle database."""
     dsn = oracledb.makedsn(host, port, service_name=service_name)
@@ -105,75 +92,118 @@ def get_oracle_connection(user, password, host, port, service_name):
 
 def data_migration_callback(ch, method, properties, body):
     database.initialize_db_pool()
-    task_info = json.loads(body)
-    task_id = task_info['task_id']
-    
-    redis_conn = get_redis_connection()
-    
-    def update_status(status, message, progress=None):
-        status_data = {"status": status, "message": message, "progress": progress}
-        redis_conn.set(f"migration_status:{task_id}", json.dumps(status_data))
+    job_message = json.loads(body)
+    job_id = job_message['job_id']
 
     try:
-        print(f" [x] Received data migration task {task_id}")
-        update_status("running", "Task received, starting migration.")
+        print(f" [x] Received data migration job {job_id} (producer)")
+        migration_job = migration_db.get_migration_job(job_id)
+        if not migration_job:
+            raise ValueError(f"Migration job {job_id} not found in database.")
+
+        migration_db.update_migration_job_status(job_id, "RUNNING")
 
         # Oracle Connection
-        ora_creds = task_info['oracle_credentials']
+        ora_creds_dict = json.loads(migration_job.source_connection_string)
         ora_conn = get_oracle_connection(
-            ora_creds['user'], ora_creds['password'], ora_creds['host'],
-            ora_creds['port'], ora_creds['service_name']
+            ora_creds_dict['user'], ora_creds_dict['password'], ora_creds_dict['host'],
+            ora_creds_dict['port'], ora_creds_dict['service_name'] or ora_creds_dict['sid']
         )
         ora_cursor = ora_conn.cursor()
 
-        # Postgres Connection
-        pg_creds = task_info['postgres_credentials']
-        pg_conn = psycopg2.connect(
-            dbname=pg_creds['dbname'], user=pg_creds['user'], password=pg_creds['password'],
-            host=pg_creds['host'], port=pg_creds['port']
-        )
-        pg_cursor = pg_conn.cursor()
-
-        source_table = task_info['source_table']
-        dest_table = task_info['destination_table']
+        source_table = migration_job.source_table_name
+        
+        # Get column names for dynamic insert
+        ora_cursor.execute(f"SELECT * FROM {source_table} WHERE ROWNUM = 0")
+        column_names = [col[0] for col in ora_cursor.description]
 
         ora_cursor.execute(f"SELECT COUNT(*) FROM {source_table}")
         total_rows = ora_cursor.fetchone()[0]
-        update_status("running", f"Found {total_rows} rows to migrate.")
+        migration_db.update_migration_job_status(job_id, "RUNNING", total_rows=total_rows)
+        print(f" [x] Job {job_id}: Found {total_rows} rows to migrate from {source_table}.")
 
         ora_cursor.execute(f"SELECT * FROM {source_table}")
         
-        rows_migrated = 0
+        rows_processed = 0
         while True:
             chunk = ora_cursor.fetchmany(CHUNK_SIZE)
             if not chunk:
                 break
             
-            # This is a simplified insert. For production, you'd need column mapping.
-            placeholders = ", ".join(['%s'] * len(chunk[0]))
-            insert_query = f"INSERT INTO {dest_table} VALUES ({placeholders})"
-            
-            pg_cursor.executemany(insert_query, chunk)
-            pg_conn.commit()
+            for row_data in chunk:
+                row_message = {
+                    "job_id": str(job_id),
+                    "row_data": list(row_data), # Convert tuple to list for JSON serialization
+                    "column_names": column_names
+                }
+                queues.get_rabbitmq_connection().channel().basic_publish(
+                    exchange='',
+                    routing_key=queues.QUEUE_CONFIG['DATA_MIGRATION_ROW']['queue'],
+                    body=json.dumps(row_message),
+                    properties=pika.BasicProperties(delivery_mode=2)
+                )
+                rows_processed += 1
             
-            rows_migrated += len(chunk)
-            progress = (rows_migrated / total_rows) * 100 if total_rows > 0 else 100
-            update_status("running", f"Migrated {rows_migrated} of {total_rows} rows.", f"{progress:.2f}%")
+            migration_db.update_migration_job_status(job_id, "RUNNING", migrated_rows=rows_processed)
 
-        update_status("completed", f"Successfully migrated {rows_migrated} rows.", "100%")
-        print(f" [x] Task {task_id} completed successfully.")
+        migration_db.update_migration_job_status(job_id, "COMPLETED", migrated_rows=rows_processed)
+        print(f" [x] Job {job_id}: All {rows_processed} rows published to queue.")
         
         ora_cursor.close()
         ora_conn.close()
+        
+        ch.basic_ack(delivery_tag=method.delivery_tag)
+
+    except Exception as e:
+        error_message = f"Data migration producer task {job_id} failed: {e}"
+        print(f" [!] {error_message}")
+        migration_db.update_migration_job_status(job_id, "FAILED", error_details=error_message)
+        ch.basic_reject(delivery_tag=method.delivery_tag, requeue=False)
+
+def data_migration_row_callback(ch, method, properties, body):
+    database.initialize_db_pool()
+    row_message = json.loads(body)
+    job_id = row_message['job_id']
+    row_data = row_message['row_data']
+    column_names = row_message['column_names']
+
+    try:
+        print(f" [x] Received data migration row for job {job_id}")
+        migration_job = migration_db.get_migration_job(job_id)
+        if not migration_job:
+            raise ValueError(f"Migration job {job_id} not found in database.")
+
+        pg_creds_dict = json.loads(migration_job.target_connection_string)
+        pg_conn = psycopg2.connect(
+            dbname=pg_creds_dict['dbname'], user=pg_creds_dict['user'], password=pg_creds_dict['password'],
+            host=pg_creds_dict['host'], port=pg_creds_dict['port']
+        )
+        pg_cursor = pg_conn.cursor()
+
+        dest_table = migration_job.target_table_name
+        
+        # Construct insert query with column names
+        columns_str = ", ".join([f'\"{col}\"' for col in column_names]) # Quote column names to handle special characters
+        placeholders = ", ".join(['%s'] * len(row_data))
+        insert_query = f"INSERT INTO {dest_table} ({columns_str}) VALUES ({placeholders})"
+        
+        pg_cursor.execute(insert_query, row_data)
+        pg_conn.commit()
+        
+        # Assuming the first column is a primary key for tracking purposes
+        source_pk_value = str(row_data[0]) if row_data else None
+        migration_db.log_migration_row_status(job_id, source_pk_value, "MIGRATED")
+        
         pg_cursor.close()
         pg_conn.close()
         
         ch.basic_ack(delivery_tag=method.delivery_tag)
 
     except Exception as e:
-        error_message = f"Data migration task {task_id} failed: {e}"
+        error_message = f"Data migration row task for job {job_id} failed: {e}"
         print(f" [!] {error_message}")
-        update_status("failed", str(e))
+        source_pk_value = str(row_data[0]) if row_data else None
+        migration_db.log_migration_row_status(job_id, source_pk_value, "FAILED", error_message)
         ch.basic_reject(delivery_tag=method.delivery_tag, requeue=False)
 
 
@@ -372,54 +402,32 @@ def main():
     channel.basic_qos(prefetch_count=1)
 
     # Setup for SQL Conversion Queue
-    channel.exchange_declare(exchange=SQL_CONVERSION_DLX, exchange_type='fanout')
-    channel.queue_declare(queue=SQL_CONVERSION_DLQ, durable=True, arguments={'x-queue-type': 'quorum'})
-    channel.queue_bind(exchange=SQL_CONVERSION_DLX, queue=SQL_CONVERSION_DLQ)
-    print(f"[*] Dead Letter Queue '{SQL_CONVERSION_DLQ}' is ready.")
-
-    channel.queue_declare(
-        queue=SQL_CONVERSION_QUEUE,
-        durable=True,
-        arguments={
-            'x-queue-type': 'quorum',
-            'x-dead-letter-exchange': SQL_CONVERSION_DLX
-        }
-    )
-    channel.basic_consume(queue=SQL_CONVERSION_QUEUE, on_message_callback=callback)
-    print(f"[*] Listening for messages on '{SQL_CONVERSION_QUEUE}'.")
-
-    # Setup for Data Migration Queue
-    channel.exchange_declare(exchange=DATA_MIGRATION_DLX, exchange_type='fanout')
-    channel.queue_declare(queue=DATA_MIGRATION_DLQ, durable=True, arguments={'x-queue-type': 'quorum'})
-    channel.queue_bind(exchange=DATA_MIGRATION_DLX, queue=DATA_MIGRATION_DLQ)
-    print(f"[*] Dead Letter Queue '{DATA_MIGRATION_DLQ}' is ready.")
-
-    channel.queue_declare(
-        queue=DATA_MIGRATION_QUEUE,
-        durable=True,
-        arguments={
-            'x-queue-type': 'quorum',
-            'x-dead-letter-exchange': DATA_MIGRATION_DLX
-        }
-    )
-    channel.basic_consume(queue=DATA_MIGRATION_QUEUE, on_message_callback=data_migration_callback)
-    print(f"[*] Listening for messages on '{DATA_MIGRATION_QUEUE}'.")
+    sql_conv_config = {
+        'queue': SQL_CONVERSION_QUEUE,
+        'dlx': SQL_CONVERSION_DLX,
+        'dlq': SQL_CONVERSION_DLQ,
+    }
+    queues.declare_quorum_queue(channel, sql_conv_config['queue'], sql_conv_config['dlx'])
+    channel.basic_consume(queue=sql_conv_config['queue'], on_message_callback=callback)
+    print(f"[*] Listening for messages on '{sql_conv_config['queue']}'.")
+
+    # Setup for initial Data Migration Job Queue (producer for row-level jobs)
+    # This queue is where the API sends the initial migration request
+    initial_data_migration_queue_name = 'data_migration_jobs' # Hardcoded for now, as it's not in QUEUE_CONFIG
+    initial_data_migration_dlx = 'data_migration_jobs_dlx'
+    queues.declare_quorum_queue(channel, initial_data_migration_queue_name, initial_data_migration_dlx)
+    channel.basic_consume(queue=initial_data_migration_queue_name, on_message_callback=data_migration_callback)
+    print(f"[*] Listening for messages on '{initial_data_migration_queue_name}'.")
+
+    # Setup for Data Migration Row Queue (consumer for row-level jobs)
+    data_migration_row_config = queues.QUEUE_CONFIG['DATA_MIGRATION_ROW']
+    queues.declare_quorum_queue(channel, data_migration_row_config['queue'], data_migration_row_config['dlx'])
+    channel.basic_consume(queue=data_migration_row_config['queue'], on_message_callback=data_migration_row_callback)
+    print(f"[*] Listening for messages on '{data_migration_row_config['queue']}'.")
 
     # Setup for SQL Execution Queue
     sql_exec_config = queues.QUEUE_CONFIG['SQL_EXECUTION']
-    channel.exchange_declare(exchange=sql_exec_config['dlx'], exchange_type='fanout')
-    channel.queue_declare(queue=sql_exec_config['dlq'], durable=True, arguments={'x-queue-type': 'quorum'})
-    channel.queue_bind(exchange=sql_exec_config['dlx'], queue=sql_exec_config['dlq'])
-    print(f"[*] Dead Letter Queue '{sql_exec_config['dlq']}' is ready.")
-
-    channel.queue_declare(
-        queue=sql_exec_config['queue'],
-        durable=True,
-        arguments={
-            'x-queue-type': 'quorum',
-            'x-dead-letter-exchange': sql_exec_config['dlx']
-        }
-    )
+    queues.declare_quorum_queue(channel, sql_exec_config['queue'], sql_exec_config['dlx'])
     channel.basic_consume(queue=sql_exec_config['queue'], on_message_callback=sql_execution_callback)
     print(f"[*] Listening for messages on '{sql_exec_config['queue']}'.")
 
